// ASRStreamService.swift - å®æ—¶æµå¼è¯­éŸ³è¯†åˆ«æœåŠ¡
// ä½¿ç”¨ WebSocket è¿æ¥åç«¯ï¼Œå®ç°è¾¹è¯´è¾¹è¯†åˆ«

import AVFoundation
import Combine
import Foundation

class ASRStreamService: NSObject, ObservableObject {
    @Published var transcript = ""
    @Published var isRecording = false
    @Published var error: String?
    @Published var audioLevel: Float = 0.0  // éŸ³é¢‘éŸ³é‡çº§åˆ« (0.0-1.0)
    @Published var isDetectingAudio = false  // æ˜¯å¦æ£€æµ‹åˆ°éŸ³é¢‘
    @Published var statusMessage: String?  // çŠ¶æ€æç¤ºæ¶ˆæ¯

    // âœ… ä½¿ç”¨ç»Ÿä¸€çš„ WebSocketClient
    private var wsClient: WebSocketClient?
    private let audioEngine = AVAudioEngine()
    private var audioFormat: AVAudioFormat?

    private var onTranscriptUpdate: ((String, Bool) -> Void)?
    private var onDeepgramReady: (() -> Void)?
    private var lastTranscriptTime: Date?  // æœ€åæ”¶åˆ°è¯†åˆ«ç»“æœçš„æ—¶é—´
    private var noAudioTimer: Timer?  // æ— éŸ³é¢‘æ£€æµ‹è®¡æ—¶å™¨
    private var deepgramConnectionTime: Date?  // Deepgram è¿æ¥æ—¶é—´
    
    // âœ… è¿æ¥çŠ¶æ€ä» WebSocketClient è·å–
    var isConnected: Bool {
        wsClient?.isConnected ?? false
    }

    override init() {
        super.init()
    }

    deinit {
        wsClient?.disconnect()
        noAudioTimer?.invalidate()
    }

    // MARK: - WebSocket è¿æ¥

    @MainActor
    func connect(language: String = "zh-CN", model: String = "nova-2") async {
        // ä½¿ç”¨ AppConfig ç»Ÿä¸€ç®¡ç†çš„ WebSocket URL
        let wsURL = AppConfig.wsASRBaseURL

        Logger.info("WebSocket URL: \(wsURL)")

        guard let url = URL(string: wsURL) else {
            self.error = "æ— æ•ˆçš„ WebSocket URL"
            Logger.error("æ— æ•ˆçš„ WebSocket URL: \(wsURL)")
            return
        }

        // âœ… ä½¿ç”¨ WebSocketClient ç»Ÿä¸€ç®¡ç†è¿æ¥
        wsClient = WebSocketClient(url: url)
        
        wsClient?.connect(
            onConnected: {
                Logger.info("ASR WebSocket è¿æ¥æˆåŠŸ")
            },
            onDisconnected: { [weak self] error in
                if let error = error {
                    Logger.error("ASR WebSocket æ–­å¼€: \(error.localizedDescription)")
                    self?.error = error.localizedDescription
                }
            },
            onMessage: { [weak self] message in
                switch message {
                case .text(let text):
                    self?.handleTextMessage(text)
                case .data:
                    // ASR ä¸å¤„ç†äºŒè¿›åˆ¶æ¶ˆæ¯
                    break
                }
            }
        )
    }

    @MainActor
    func disconnect() async {
        guard isConnected else { return }

        // å‘é€åœæ­¢æ¶ˆæ¯
        try? await wsClient?.send(json: ["type": "stop"])

        // âœ… ä½¿ç”¨ WebSocketClient æ–­å¼€
        wsClient?.disconnect()
        wsClient = nil

        Logger.info("ASR WebSocket è¿æ¥å·²å…³é—­")
    }

    // MARK: - æ¶ˆæ¯å¤„ç†

    private func handleTextMessage(_ text: String) {
        guard let data = text.data(using: .utf8),
            let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
            let type = json["type"] as? String
        else {
            return
        }

        Task { @MainActor in
            switch type {
            case "connected":
                Logger.info("WebSocket è¿æ¥æˆåŠŸ")
                
            case "connecting":
                let message = json["message"] as? String ?? "æ­£åœ¨è¿æ¥ Deepgram..."
                Logger.info("ğŸ“¡ \(message)")
                self.statusMessage = "ğŸ“¡ æ­£åœ¨è¿æ¥è¯­éŸ³è¯†åˆ«æœåŠ¡..."

            case "started":
                Logger.info("è¯†åˆ«å·²å¯åŠ¨ï¼ŒDeepgram å‡†å¤‡å°±ç»ª")
                self.deepgramConnectionTime = Date()
                self.statusMessage = "ğŸ¤ å¼€å§‹è¯´è¯..."
                
                // å¯åŠ¨æ— éŸ³é¢‘æ£€æµ‹è®¡æ—¶å™¨ï¼ˆ15ç§’åå¦‚æœæ²¡æœ‰è¯†åˆ«ç»“æœï¼Œç»™å‡ºæç¤ºï¼‰
                self.startNoAudioDetectionTimer()
                
                // é€šçŸ¥ Deepgram å·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹å½•éŸ³
                self.onDeepgramReady?()

            case "transcript":
                let transcript = json["transcript"] as? String ?? ""
                let isFinal = json["is_final"] as? Bool ?? false
                let confidence = json["confidence"] as? Double ?? 0

                Logger.info("è¯†åˆ«ç»“æœ: \(transcript) [isFinal: \(isFinal), confidence: \(confidence)]")

                // æ›´æ–°æœ€åè¯†åˆ«æ—¶é—´
                self.lastTranscriptTime = Date()
                
                // æ¸…é™¤çŠ¶æ€æ¶ˆæ¯
                self.statusMessage = nil
                
                // é‡ç½®æ— éŸ³é¢‘æ£€æµ‹è®¡æ—¶å™¨
                self.resetNoAudioDetectionTimer()

                // æ›´æ–°æ–‡æœ¬
                self.transcript = transcript

                // è°ƒç”¨å›è°ƒ
                self.onTranscriptUpdate?(transcript, isFinal)

            case "stopped":
                Logger.info("è¯†åˆ«å·²åœæ­¢")
                self.isRecording = false

            case "deepgram_closed":
                let message = json["message"] as? String
                Logger.info("Deepgram è¿æ¥å·²å…³é—­: \(message ?? "")")
                self.isRecording = false
                
                // å¦‚æœæ˜¯åœ¨å½•éŸ³è¿‡ç¨‹ä¸­æ–­å¼€ï¼ˆéä¸»åŠ¨åœæ­¢ï¼‰ï¼Œæ˜¾ç¤ºè­¦å‘Š
                if self.isRecording {
                    self.statusMessage = "âš ï¸ è¯­éŸ³è¯†åˆ«æœåŠ¡å·²æ–­å¼€ï¼Œè¯·é‡æ–°å¼€å§‹"
                } else {
                    // ä¸»åŠ¨åœæ­¢çš„æƒ…å†µï¼Œä¸æ˜¾ç¤ºé”™è¯¯
                    self.statusMessage = nil
                }
                
                self.stopNoAudioDetectionTimer()

            case "error":
                let errorMsg = json["message"] as? String ?? "Unknown error"
                let originalError = json["original_error"] as? String
                
                Logger.error("æœåŠ¡å™¨é”™è¯¯: \(errorMsg)")
                if let originalError = originalError {
                    Logger.error("åŸå§‹é”™è¯¯: \(originalError)")
                }
                
                self.error = errorMsg
                
                // æ ¹æ®é”™è¯¯ç±»å‹æ˜¾ç¤ºä¸åŒçš„çŠ¶æ€æ¶ˆæ¯
                if errorMsg.contains("API") || errorMsg.contains("è®¤è¯") {
                    self.statusMessage = "âŒ API é…ç½®é”™è¯¯ï¼Œè¯·è”ç³»ç®¡ç†å‘˜"
                } else if errorMsg.contains("ç½‘ç»œ") || errorMsg.contains("è¿æ¥") || errorMsg.contains("è¶…æ—¶") {
                    self.statusMessage = "âŒ ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ"
                } else if errorMsg.contains("DNS") {
                    self.statusMessage = "âŒ ç½‘ç»œé…ç½®é”™è¯¯"
                } else if errorMsg.contains("ä¸å¯ç”¨") || errorMsg.contains("503") {
                    self.statusMessage = "âŒ æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•"
                } else if errorMsg.contains("é¢‘ç‡") || errorMsg.contains("è¶…é™") {
                    self.statusMessage = "âŒ ä½¿ç”¨é¢‘ç‡è¿‡é«˜ï¼Œè¯·ç¨åå†è¯•"
                } else {
                    self.statusMessage = "âŒ \(errorMsg)"
                }
                
                self.stopNoAudioDetectionTimer()

            case "pong":
                // å¿ƒè·³å“åº”
                break

            default:
                Logger.info("æœªçŸ¥æ¶ˆæ¯ç±»å‹: \(type)")
            }
        }
    }

    private func sendMessage(_ message: [String: Any]) async {
        // âœ… ä½¿ç”¨ WebSocketClient å‘é€æ¶ˆæ¯
        do {
            try await wsClient?.send(json: message)
        } catch {
            Logger.error("å‘é€æ¶ˆæ¯å¤±è´¥: \(error.localizedDescription)")
        }
    }

    // MARK: - å½•éŸ³æ§åˆ¶

    @MainActor
    func startRecording(
        language: String = "zh-CN",
        model: String = "nova-2",
        onDeepgramReady: @escaping () -> Void,
        onTranscriptUpdate: @escaping (String, Bool) -> Void
    ) {
        // ä¿å­˜å›è°ƒï¼Œç­‰å¾… Deepgram å°±ç»ªåå†å¯åŠ¨éŸ³é¢‘å¼•æ“
        self.onDeepgramReady = { [weak self] in
            guard let self = self else { return }
            // Deepgram å°±ç»ªï¼Œå¯åŠ¨éŸ³é¢‘å¼•æ“
            Task { @MainActor in
                self.startAudioEngine()
                // è°ƒç”¨å¤–éƒ¨çš„å°±ç»ªå›è°ƒ
                onDeepgramReady()
            }
        }
        self.onTranscriptUpdate = onTranscriptUpdate

        guard isConnected else {
            self.error = "WebSocket æœªè¿æ¥"
            return
        }

        // å‘é€ start æ¶ˆæ¯ï¼Œè§¦å‘ Deepgram è¿æ¥
        Task {
            let startMessage: [String: Any] = [
                "type": "start",
                "language": language,
                "model": model,
            ]
            await sendMessage(startMessage)
            Logger.info("å·²å‘é€ start æ¶ˆæ¯ï¼Œç­‰å¾… Deepgram å°±ç»ª...")
        }
    }

    // å¯åŠ¨éŸ³é¢‘å¼•æ“ï¼ˆDeepgram å°±ç»ªåè°ƒç”¨ï¼‰
    @MainActor
    private func startAudioEngine() {
        // é…ç½®éŸ³é¢‘ä¼šè¯
        let audioSession = AVAudioSession.sharedInstance()
        do {
            try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
            try audioSession.setActive(true)
        } catch {
            Logger.error("éŸ³é¢‘ä¼šè¯é…ç½®å¤±è´¥: \(error)")
            self.error = "éŸ³é¢‘ä¼šè¯é…ç½®å¤±è´¥"
            self.statusMessage = "âŒ éº¦å…‹é£é…ç½®å¤±è´¥"
            return
        }

        // é…ç½®éŸ³é¢‘æ ¼å¼ï¼š16kHz, å•å£°é“, 16-bit PCM
        let inputNode = audioEngine.inputNode
        let inputFormat = inputNode.outputFormat(forBus: 0)

        // åˆ›å»ºç›®æ ‡æ ¼å¼ï¼š16kHz, Int16, Interleavedï¼ˆDeepgram è¦æ±‚ï¼‰
        guard
            let targetFormat = AVAudioFormat(
                commonFormat: .pcmFormatInt16,
                sampleRate: 16000,
                channels: 1,
                interleaved: true  // âœ… Deepgram éœ€è¦äº¤é”™æ ¼å¼
            )
        else {
            self.error = "æ— æ³•åˆ›å»ºéŸ³é¢‘æ ¼å¼"
            self.statusMessage = "âŒ éŸ³é¢‘æ ¼å¼é”™è¯¯"
            return
        }

        self.audioFormat = targetFormat

        // åˆ›å»ºæ ¼å¼è½¬æ¢å™¨
        guard let converter = AVAudioConverter(from: inputFormat, to: targetFormat) else {
            self.error = "æ— æ³•åˆ›å»ºéŸ³é¢‘è½¬æ¢å™¨"
            self.statusMessage = "âŒ éŸ³é¢‘è½¬æ¢å™¨é”™è¯¯"
            return
        }

        // å®‰è£…éŸ³é¢‘é‡‡é›†
        inputNode.installTap(onBus: 0, bufferSize: 4096, format: inputFormat) { [weak self] buffer, _ in
            guard let self = self else { return }

            // è®¡ç®—éŸ³é¢‘éŸ³é‡çº§åˆ«
            self.calculateAudioLevel(buffer: buffer)
            
            // è½¬æ¢éŸ³é¢‘æ ¼å¼
            self.convertAndSendAudio(buffer: buffer, converter: converter, targetFormat: targetFormat)
        }

        // å¯åŠ¨éŸ³é¢‘å¼•æ“
        do {
            try audioEngine.start()
            isRecording = true
            Logger.info("âœ… éŸ³é¢‘å¼•æ“å·²å¯åŠ¨ï¼Œå¼€å§‹å½•éŸ³")
        } catch {
            Logger.error("éŸ³é¢‘å¼•æ“å¯åŠ¨å¤±è´¥: \(error)")
            self.error = "æ— æ³•å¯åŠ¨å½•éŸ³"
            self.statusMessage = "âŒ æ— æ³•å¯åŠ¨å½•éŸ³"
        }
    }

    @MainActor
    func stopRecording() {
        guard isRecording else {
            Logger.debug("å½•éŸ³æœªåœ¨è¿›è¡Œä¸­ï¼Œå¿½ç•¥åœæ­¢è¯·æ±‚")
            return
        }

        // åœæ­¢éŸ³é¢‘å¼•æ“
        if audioEngine.isRunning {
            audioEngine.stop()
            audioEngine.inputNode.removeTap(onBus: 0)
        }

        isRecording = false
        audioLevel = 0.0
        isDetectingAudio = false
        statusMessage = nil
        
        // åœæ­¢æ— éŸ³é¢‘æ£€æµ‹è®¡æ—¶å™¨
        stopNoAudioDetectionTimer()

        // å‘é€ stop æ¶ˆæ¯ï¼Œè®©æœåŠ¡å™¨æ–­å¼€ Deepgram
        Task {
            let stopMessage: [String: Any] = ["type": "stop"]
            await sendMessage(stopMessage)
            Logger.info("åœæ­¢å½•éŸ³ï¼Œå·²å‘é€ stop æ¶ˆæ¯")
        }
    }

    // MARK: - éŸ³é¢‘éŸ³é‡æ£€æµ‹
    
    private func calculateAudioLevel(buffer: AVAudioPCMBuffer) {
        guard let channelData = buffer.floatChannelData?[0] else { return }
        
        let frameLength = Int(buffer.frameLength)
        var sum: Float = 0.0
        
        // è®¡ç®— RMSï¼ˆå‡æ–¹æ ¹ï¼‰
        for i in 0..<frameLength {
            let sample = channelData[i]
            sum += sample * sample
        }
        
        let rms = sqrt(sum / Float(frameLength))
        let db = 20 * log10(rms)
        
        // å½’ä¸€åŒ–åˆ° 0-1 èŒƒå›´ï¼ˆ-60dB åˆ° 0dBï¼‰
        let normalizedLevel = max(0, min(1, (db + 60) / 60))
        
        Task { @MainActor in
            self.audioLevel = normalizedLevel
            
            // æ£€æµ‹æ˜¯å¦æœ‰å£°éŸ³ï¼ˆé˜ˆå€¼ 0.1ï¼‰
            let hasAudio = normalizedLevel > 0.1
            if hasAudio != self.isDetectingAudio {
                self.isDetectingAudio = hasAudio
                if hasAudio {
                    Logger.debug("ğŸ¤ æ£€æµ‹åˆ°å£°éŸ³ï¼ŒéŸ³é‡: \(normalizedLevel)")
                }
            }
        }
    }
    
    // MARK: - æ— éŸ³é¢‘æ£€æµ‹è®¡æ—¶å™¨
    
    private func startNoAudioDetectionTimer() {
        stopNoAudioDetectionTimer()
        
        noAudioTimer = Timer.scheduledTimer(withTimeInterval: 5.0, repeats: true) { [weak self] _ in
            guard let self = self else { return }
            
            Task { @MainActor in
                // æ£€æŸ¥æ˜¯å¦é•¿æ—¶é—´æ²¡æœ‰è¯†åˆ«ç»“æœ
                if let lastTime = self.lastTranscriptTime {
                    let timeSinceLastTranscript = Date().timeIntervalSince(lastTime)
                    if timeSinceLastTranscript > 10 {
                        if self.isDetectingAudio {
                            self.statusMessage = "ğŸ”Š æ£€æµ‹åˆ°å£°éŸ³ä½†æ— æ³•è¯†åˆ«ï¼Œè¯·è¯´æ¸…æ¥šä¸€ç‚¹"
                        } else {
                            self.statusMessage = "ğŸ¤” æ²¡æœ‰æ£€æµ‹åˆ°å£°éŸ³ï¼Œè¯·é è¿‘éº¦å…‹é£è¯´è¯"
                        }
                    }
                } else if let connectionTime = self.deepgramConnectionTime {
                    let timeSinceConnection = Date().timeIntervalSince(connectionTime)
                    if timeSinceConnection > 8 {
                        if self.isDetectingAudio {
                            self.statusMessage = "ğŸ”Š æ£€æµ‹åˆ°å£°éŸ³ä½†æ— æ³•è¯†åˆ«ï¼Œè¯·è¯´æ¸…æ¥šä¸€ç‚¹"
                        } else {
                            self.statusMessage = "ğŸ¤” æ²¡æœ‰æ£€æµ‹åˆ°å£°éŸ³ï¼Œè¯·é è¿‘éº¦å…‹é£è¯´è¯"
                        }
                    }
                }
            }
        }
    }
    
    private func resetNoAudioDetectionTimer() {
        // é‡æ–°å¯åŠ¨è®¡æ—¶å™¨
        startNoAudioDetectionTimer()
    }
    
    private func stopNoAudioDetectionTimer() {
        noAudioTimer?.invalidate()
        noAudioTimer = nil
    }
    
    // MARK: - éŸ³é¢‘å¤„ç†

    private func convertAndSendAudio(
        buffer: AVAudioPCMBuffer,
        converter: AVAudioConverter,
        targetFormat: AVAudioFormat
    ) {
        // åˆ›å»ºè¾“å‡ºç¼“å†²åŒº
        let capacity = AVAudioFrameCount(
            Double(buffer.frameLength) * targetFormat.sampleRate / buffer.format.sampleRate
        )
        guard
            let convertedBuffer = AVAudioPCMBuffer(
                pcmFormat: targetFormat,
                frameCapacity: capacity
            )
        else {
            return
        }

        var error: NSError?
        let inputBlock: AVAudioConverterInputBlock = { _, outStatus in
            outStatus.pointee = .haveData
            return buffer
        }

        converter.convert(to: convertedBuffer, error: &error, withInputFrom: inputBlock)

        if let error = error {
            Logger.error("éŸ³é¢‘è½¬æ¢å¤±è´¥: \(error)")
            return
        }

        // è½¬æ¢ä¸º Data
        guard let audioData = bufferToData(convertedBuffer) else {
            return
        }

        // å‘é€éŸ³é¢‘æ•°æ®
        Task {
            await sendAudioData(audioData)
        }
    }

    private func bufferToData(_ buffer: AVAudioPCMBuffer) -> Data? {
        guard let channelData = buffer.int16ChannelData else {
            return nil
        }

        let channelDataPointer = channelData.pointee
        let dataSize = Int(buffer.frameLength) * MemoryLayout<Int16>.size

        return Data(bytes: channelDataPointer, count: dataSize)
    }

    private func sendAudioData(_ data: Data) async {
        // âœ… ä½¿ç”¨ WebSocketClient å‘é€éŸ³é¢‘æ•°æ®
        do {
            try await wsClient?.send(data: data)
        } catch {
            // å¿½ç•¥å‘é€é”™è¯¯ï¼Œé¿å…æ—¥å¿—åˆ·å±
        }
    }

}
